
-- =========================================================
-- Project : Laptop Dataset Cleaning
-- Author  : Kedar Raju Pawar
-- Date    : 14-02-2026
-- Time    : 00:00:01

-- ðŸ” Duplicate & Integrity Checks

-- 38. How would you detect duplicate laptop records?
-- I would group by the core descriptive columns that define a laptop configuration and look for groups with count > 1. That shows true business duplicates, not just repeated IDs.
SELECT
    Company,
    laptop_type,
    screen_size_inch,
    cpu,
    Ram,
    Memory,
    gpu,
    OpSys,
    price,
    COUNT(*) AS dup_count
FROM cleaned_laptop_data
GROUP BY
    Company, laptop_type, screen_size_inch,
    cpu, Ram, Memory, gpu, OpSys, price
HAVING COUNT(*) > 1
ORDER BY dup_count DESC;
-- 39. What column combination would you use as a uniqueness key?
-- I would use a composite business key based on hardware configuration â€” brand + CPU + RAM + storage + GPU + screen size â€” because together they define the product spec better than any single column.
-- Candidate Key Columns:
-- Company, screen_size_inch, cpu, ram_gb
-- total_storage_gb, gpu, display_resolution_width, display_resolution_height
SELECT COUNT(*) total_rows,
COUNT(DISTINCT CONCAT_WS('|',
    Company, screen_size_inch, cpu,
    ram_gb, total_storage_gb, gpu
)) distinct_configs
FROM cleaned_laptop_data;
-- 40. How would you keep the best record among duplicates instead of deleting blindly?

 -- Instead of deleting duplicates randomly, I would rank them and keep the most complete record â€” for example, the one with fewer NULL fields or 
 -- more parsed attributes populated.
 SELECT *
FROM (
    SELECT *,
           ROW_NUMBER() OVER (
             PARTITION BY Company, cpu, ram_gb, total_storage_gb
             ORDER BY
               (cpu_model IS NOT NULL) +
               (clock_speed_ghz IS NOT NULL) +
               (panel_type IS NOT NULL) DESC
           ) AS rn
    FROM cleaned_laptop_data
) t
WHERE rn = 1;
 
-- ðŸ§ª Data Quality Flags (Advanced Analyst Move)

-- 41. How would you create a flag column for â€œsuspect recordâ€ where:
-- high RAM
-- very low price
-- premium CPU
-- but budget cost
-- Create a rule-based flag where specs look premium but price is abnormally low. This helps detect scraping errors or unit mistakes.
ALTER TABLE cleaned_laptop_data
ADD COLUMN suspect_flag VARCHAR(3);

UPDATE cleaned_laptop_data
SET suspect_flag =
CASE
 WHEN ram_gb >= 32
  AND cpu_tier = 'High'
  AND price < 30000
 THEN 'Yes'
 ELSE 'No'
END;

select suspect_flag,count(*)
from cleaned_laptop_data
group by suspect_flag;
-- 42. How would you create a â€œdata_quality_scoreâ€ column based on missing fields?
-- Score each row by counting how many important fields are populated. Higher score = better record quality.
ALTER TABLE cleaned_laptop_data
ADD COLUMN data_quality_score INT;

UPDATE cleaned_laptop_data
SET data_quality_score =
    (cpu_brand IS NOT NULL) +
    (cpu_model IS NOT NULL) +
    (ram_gb IS NOT NULL) +
    (total_storage_gb IS NOT NULL) +
    (gpu_brand IS NOT NULL) +
    (display_resolution_width IS NOT NULL);
    
select data_quality_score,count(*)
from cleaned_laptop_data
group by data_quality_score;
-- 42. How would you measure % completeness per column?
-- Measure completeness by calculating non-NULL percentage per column relative to total rows.

SELECT
  ROUND(100 * COUNT(ram_gb) / COUNT(*),2) AS ram_completeness_pct,
  ROUND(100 * COUNT(cpu_model) / COUNT(*),2) AS cpu_model_pct,
  ROUND(100 * COUNT(total_storage_gb) / COUNT(*),2) AS storage_pct
FROM cleaned_laptop_data;
-- ðŸ—ï¸ Cleaning Architecture Questions (Interview Gold)

-- 43. When should cleaning be done in SQL vs Python?
-- SQL â†’ structural cleaning, type casting, parsing columns, deduplication, joins, aggregations

-- Python â†’ complex logic, fuzzy matching, NLP text normalization, ML-based anomaly detection, multi-step conditional parsing
-- Rule of thumb: If itâ€™s set-based and deterministic â†’ SQL. If itâ€™s algorithmic â†’ Python

-- 44. Why keep raw table and cleaned table separate?
-- preserves source truth, enables rollback , allows audit trails

-- supports reproducibility, prevents irreversible damage

-- Raw is evidence. Cleaned is interpretation. Never mix them.

-- 45. Why should derived columns never overwrite raw columns?
-- Because parsing rules evolve. If you overwrite raw text, you destroy the ability to re-derive with improved logic. 
-- Derived columns are versioned outputs â€” raw columns are permanent inputs.
UPDATE cleaned_laptop_data
SET cpu_generation = REGEXP_SUBSTR(cpu, 'i[3579]-([0-9]{4})', 1, 1, NULL, 1)
WHERE cpu_brand = 'Intel' AND cpu REGEXP 'i[3579]-';

-- 46. How would you make your cleaning steps reproducible?
-- store all cleaning SQL in scripts, run in ordered steps, use version control (Git), avoid manual edits
-- parameterize thresholds , document assumptions, validate with check queries
-- Reproducible cleaning = same input â†’ same output every run. Thatâ€™s professional analytics.
